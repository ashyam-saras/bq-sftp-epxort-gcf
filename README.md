# GCS to SFTP Export Service

This service uploads pre-exported data from Google Cloud Storage (GCS) to an SFTP server. Data is assumed to be generated by upstream BigQuery Scheduled Queries (or other producers). The service runs on Cloud Run and can be triggered via HTTP requests or Pub/Sub messages.

## Features

- **GCS → SFTP**: Moves files already present in GCS to SFTP
- **Cloud Run Service**: Runs as a scalable, containerized service
- **Pub/Sub Integration**: Can be triggered via Pub/Sub messages for asynchronous processing
- **Cloud Scheduler**: Use Scheduler to trigger Cloud Run or Pub/Sub
- **Configurable Sources**: Define GCS prefixes or patterns per export
- **Parallel Transfer**: Multi-threaded uploads for improved performance
- **Structured Logging**: JSON-formatted logs for better observability

## Configuration

The service uses a JSON configuration file to define export settings. Each export must define a `gcs_source` with exactly one of `pattern` or `prefix`. A `{date}` token can be used and will be expanded to `YYYYMMDD` at runtime.

```json
{
  "sftp": {
    "host": "sftp.example.com",
    "port": 22,
    "username": "user",
    "password": "password",
    "directory": "/uploads"
  },
  "exports": {
    "product_data": {
      "gcs_source": {
        "pattern": "gs://my-bucket/scheduled/products/{date}/products-{date}-*.csv.gz"
      }
    },
    "customer_data": {
      "gcs_source": {
        "prefix": "gs://my-bucket/scheduled/customers/{date}/",
        "pattern": "*.csv.gz"
      }
    }
  }
}
```

- **pattern**: Full `gs://` path that can include `*` wildcards (e.g., shard names) and `{date}`.
- **prefix**: A `gs://` folder prefix. Optional `pattern` can filter filenames within the prefix.
- The service forbids legacy BigQuery query options (`source_table`, `date_column`, `days_lookback`, `compress`, `export_type`).

## Project Structure

```
bq-sftp-export/
├── configs/
│   └── default.json                # Default export configuration
├── scripts/
│   ├── failed_transfers.py         # Utility to detect failed transfers
│   ├── retry_transfers.py          # Utility to retry failed transfers
│   ├── setup_cloudrun.sh           # Script to deploy to Cloud Run
│   ├── setup_pubsub.sh             # Script to set up Pub/Sub integration
│   └── setup_schedulers.sh         # Script to set up Cloud Scheduler jobs
├── src/
│   ├── __init__.py
│   ├── config.py                   # Configuration handling (GCS-only)
│   ├── helpers.py                  # Helper functions
│   ├── main.py                     # Orchestration: list GCS → upload to SFTP
│   └── sftp.py                     # SFTP operations
├── tests/
│   ├── __init__.py
│   ├── test_config.py
│   ├── test_helpers.py
│   ├── test_main.py
│   └── test_sftp.py
├── Dockerfile                      # Container definition
├── README.md
├── requirements.txt                # Production dependencies
├── requirements-dev.txt            # Development dependencies
└── server.py                       # HTTP server for Cloud Run
```

## Deployment

### Local Development

```bash
# Set up virtual environment
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
pip install -r requirements-dev.txt

# Set required environment variables
export GOOGLE_APPLICATION_CREDENTIALS="path/to/service-account-key.json"
export SFTP_HOST="sftp.example.com"
export SFTP_USERNAME="username"
export SFTP_PASSWORD="password"
export SFTP_DIRECTORY="/uploads"

# (Optional) Use a local config file
# cp configs/default.json.sample configs/default.json

# Run the server locally
python server.py
```

### Build and Deploy to Cloud Run

```bash
chmod +x scripts/setup_cloudrun.sh
./scripts/setup_cloudrun.sh
```

### Setting Up Pub/Sub Integration

```bash
chmod +x scripts/setup_pubsub.sh  
./scripts/setup_pubsub.sh
```

### Setting Up Cloud Scheduler

```bash
chmod +x scripts/setup_schedulers.sh
./scripts/setup_schedulers.sh
```

## Usage

### Triggering Exports via HTTP

```bash
curl -X POST https://your-service-url/ \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $(gcloud auth print-identity-token)" \
  -d '{"export_name":"product_data","date":"2025-03-17"}'
```

- `export_name`: must match a key under `exports` in config
- `date` (optional): expands `{date}` tokens in GCS paths to `YYYYMMDD`

### Triggering Exports via Pub/Sub

```bash
gcloud pubsub topics publish boxout-sftp-export-triggers \
  --message='{"export_name":"product_data","date":"2025-03-17"}'
```

## Local Debugging and Testing

### 1) Dry-run listing
Use Python to list files that would be transferred for a given export/date.
```bash
python -c "from src.config import load_config; from src.main import _resolve_date_token,_list_gcs_files_by_pattern,_list_gcs_files_by_prefix; from google.cloud import storage; import datetime; cfg=load_config('configs/default.json'); e=cfg['exports']['product_data']; d=datetime.date.fromisoformat('2025-03-17'); pat=e['gcs_source'].get('pattern'); pre=e['gcs_source'].get('prefix'); sc=storage.Client(); pat=_resolve_date_token(pat,d) if pat else None; pre=_resolve_date_token(pre,d) if pre else None; blobs=_list_gcs_files_by_pattern(sc, pat) if pat else _list_gcs_files_by_prefix(sc, pre, e['gcs_source'].get('pattern')); print('\n'.join([b.name for b in blobs]))"
```

### 2) CLI run
Run a single export locally:
```bash
python -m src.main --export product_data --date 2025-03-17 --config configs/default.json
```

### 3) Server run and curl
```bash
python server.py
curl -X POST http://localhost:8080/ \
  -H 'Content-Type: application/json' \
  -d '{"export_name":"product_data","date":"2025-03-17"}'
```

### 4) Troubleshooting tips
- Ensure `GOOGLE_APPLICATION_CREDENTIALS` points to a service account with access to GCS.
- Verify GCS paths exist and `{date}` resolves to the expected folder (YYYYMMDD).
- Check Cloud Run logs or local stdout for structured logs.
- SFTP connectivity: test with `python -m src.sftp check` using env vars for host/user/password.

## Monitoring

- Cloud Run logs / Cloud Logging (primary source of truth)
- Cloud Scheduler execution history
- Pub/Sub message delivery logs
